{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa3eaf8-da13-491c-ae10-417109a4fc89",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3321001340.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [1]\u001b[0;36m\u001b[0m\n\u001b[0;31m    - Evaluation Metric is an essential part in any Machine Learning project.\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Metrics with Scikit-Learn\n",
    "\n",
    "- Evaluation Metric is an essential part in any Machine Learning project.\n",
    "- It measures how good or bad is your Machine Learning model\n",
    "- Different Evaluation Metrics are used for Regression model (Continuous output) or Classification model (Categorical output).\n",
    "\n",
    "## 4.1 Regression model Evaluation Metrics\n",
    "\n",
    "### 4.1.1 Correlation Coefficient (R) or Coefficient of Determination (R2):\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/120700259-72274900-c47f-11eb-8959-a4bbe4eafccc.png)\n",
    "\n",
    "```python\n",
    "from sklearn import metrics\n",
    "metrics.r2_score(y_test,y_pred)\n",
    "```\n",
    "\n",
    "### 4.1.2 Root Mean Square Error (RMSE) or Mean Square Error (MSE)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/120700533-c5010080-c47f-11eb-8050-b1cd8c63746e.png)\n",
    "\n",
    "```python\n",
    "from sklearn import metrics\n",
    "print(metrics.mean_squared_error(y_test,y_pred,squared=False)) # RMSE\n",
    "print(metrics.mean_squared_error(y_test,y_pred,squared=True)) # MSE\n",
    "```\n",
    "\n",
    "## 4.2. Classification model Evaluation Metrics\n",
    "\n",
    "### 4.2.1 Confusion Matrix\n",
    "- A confusion matrix is a technique for summarizing the performance of a classification algorithm.\n",
    "- You can learn more about Confusion Matrix [here](https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/)\n",
    "\n",
    "For binary output (classification problem with only 2 output type, also most popular):\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/120687356-efe35880-c46f-11eb-950f-5feef237a4c1.png)\n",
    "\n",
    "### 4.2.2 Accuracy\n",
    "\n",
    "The most common metric for classification is accuracy, which is the fraction of samples predicted correctly as shown below:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/120700619-dea24800-c47f-11eb-81c4-df090cad93da.png)\n",
    "\n",
    "```python\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test,y_pred)\n",
    "```\n",
    "\n",
    "### 4.2.3 Precision \n",
    "\n",
    "Precision is the fraction of predicted positives events that are actually positive as shown below:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/120700808-1c9f6c00-c480-11eb-9ec8-597d02a76a94.png)\n",
    "\n",
    "### 4.2.4 Recall\n",
    "\n",
    "Recall (also known as sensitivity) is the fraction of positives events that you predicted correctly as shown below:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/120700754-07c2d880-c480-11eb-81e1-7c7926452346.png)\n",
    "\n",
    "\n",
    "### 4.2.5 F1 score\n",
    "\n",
    "The f1 score is the harmonic mean of recall and precision, with a higher score as a better model. The f1 score is calculated using the following formula:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/120701061-6ee08d00-c480-11eb-9ab1-71d905e6a491.png)\n",
    "\n",
    "More information on Precision, Recall and F1 score can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)\n",
    "\n",
    "```python\n",
    "metrics.precision_recall_fscore_support(y_test,y_pred,average='binary')\n",
    "```\n",
    "\n",
    "### 4.2.6 AUC-ROC curve\n",
    "- ROC: Receiver Operating Characteristics:  probability curve\n",
    "- AUC: Area Under The Curve: represents the degree or measure of separability.\n",
    " \n",
    "![image](https://user-images.githubusercontent.com/43855029/120698991-ccbfa580-c47d-11eb-9f11-6e2acb00d46d.png)\n",
    "\n",
    "  - AUC = 1:   perfect prediction\n",
    "  - AUC = 0.8: model has 80% chance to predict the right class\n",
    "  - AUC = 0.5: worst case, model has **NO** accuracy in prediction (random)\n",
    "  - AUC = 0:   the model is actually reciprocating the classes\n",
    "  \n",
    "![image](https://user-images.githubusercontent.com/43855029/120699552-84ed4e00-c47e-11eb-8089-54158439ad6f.png)\n",
    "\n",
    "ROC Interpretation\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/133898061-2c7f5da6-c41b-41af-8a81-b65fef3c3184.png)\n",
    "\n",
    "Code to calculate FPR, TPR:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test,y_pred)\n",
    "```\n",
    "\n",
    "Code to calculate AUC score:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_score = roc_auc_score(y_test,y_pred)\n",
    "```\n",
    "\n",
    "We will go into detail how to plot AUC-ROC curve in the next chapter with a classification problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abccc7e-9208-4a96-801f-f62e84642bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
